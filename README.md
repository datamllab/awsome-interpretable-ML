# Awsome-Interpretable-ML

A curated, but probably biased and incomplete, list of awesome Trojan Attack in AI resources.

If you want to contribute to this list, feel free to pull a request. Also you can contact [Ninghao Liu](http://people.tamu.edu/~nhliu43/) from the [Data Lab](http://faculty.cs.tamu.edu/xiahu/) at Texas A&M University through email: nhliu43@tamu.edu.

## Benchmark Datasets
* [Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words? (ACL20)](https://www.aclweb.org/anthology/2020.acl-main.419.pdf)

## Local Interpretation in Computer Vision


## Local Interpretation in NLP
* [Attention is not Explanation (NAACL19)](https://www.aclweb.org/anthology/N19-1357/) <br />
* [Attention is not not Explanation (EMNLP19)](https://www.aclweb.org/anthology/D19-1002) <br />
* [Is Attention Interpretable? (ACL19)](https://arxiv.org/pdf/1906.03731.pdf) <br />

## Local Interpretation in Graph Analytics
* [GNNExplainer: Generating Explanations for Graph Neural Networks (NeurIPS19)](http://papers.nips.cc/paper/9123-gnnexplainer-generating-explanations-for-graph-neural-networks.pdf)

## Representation Interpretation
* [On Interpretation of Network Embedding via Taxonomy Induction (KDD18)](https://dl.acm.org/doi/10.1145/3219819.3220001)
* [Interpretable Basis Decomposition for Visual Explanation (ECCV18)](https://openaccess.thecvf.com/content_ECCV_2018/papers/Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper.pdf)
* [Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (ICML19)](https://arxiv.org/pdf/1711.11279.pdf)
* [Towards a deep and unified understanding of deep neural models in nlp (ICML19)](http://proceedings.mlr.press/v97/guan19a/guan19a.pdf)

## Interpretable Modeling


## Application of ML Interpretation
* [Learning Credible Deep Neural Networks with Rationale Regularization (ICDM19)](https://arxiv.org/pdf/1908.05601.pdf) <br />
